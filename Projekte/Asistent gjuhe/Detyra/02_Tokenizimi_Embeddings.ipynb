{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Detyra 2 — Tokenizimi & Embeddings\n\nQëllimi: të ndërtoni embedding-e për *njësi semantike* (fraza, dialogë, gabime) dhe të matni ngjashmëri.\n\n⚠️ Shënim: Në këtë projekt, dokumentet janë PDF. Për thjeshtësi në kurs, ju mund:\n1) të përdorni tekstin e dokumenteve (nëse instruktori ofron .txt/.md paralel), ose\n2) të ekstraktoni tekst me PyMuPDF (fitz) / pdfminer.six.\n\nNë këtë notebook ne japim një shembull minimal me PyMuPDF dhe lëmë TODO për pastrim/segmentim.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# === Setup (run this cell first) ===\nimport os, json, re, math, random\nfrom pathlib import Path\n\nPROJECT_ROOT = Path(\"/content\")  # In Colab, students will upload the project folder or mount Drive\nDOCS_DIR = PROJECT_ROOT / \"documents\"\nQA_PATH = PROJECT_ROOT / \"qa\" / \"qa_benchmark_40.json\"\n\nprint(\"Docs dir:\", DOCS_DIR)\nprint(\"QA path:\", QA_PATH)\n\n# Tip: If you're using Google Drive:\n# from google.colab import drive\n# drive.mount('/content/drive')\n# PROJECT_ROOT = Path('/content/drive/MyDrive/<YOUR_PROJECT_FOLDER>')\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Ekstraktim minimal i tekstit nga një PDF\nZgjidhni një PDF dhe ekstraktoni tekstin. Pastaj bëni pastrim bazë.\n**TODO:** përmirësoni pastrimin (heqje header/footer, normalizim hapësirash).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Install PyMuPDF if needed\n!pip -q install pymupdf\n\nimport fitz, re\nfrom pathlib import Path\n\npdf_path = next(iter(sorted(DOCS_DIR.glob(\"MIL-ENG-001*.pdf\"))), None)\nprint(\"Using:\", pdf_path)\n\ndoc = fitz.open(str(pdf_path))\ntext = \"\"\nfor page in doc:\n    text += page.get_text() + \"\\n\"\ntext = re.sub(r\"\\s+\", \" \", text).strip()\nprint(\"Chars:\", len(text))\nprint(text[:800])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Segmentim në 'njësi semantike'\nNe synojmë të ndajmë tekstin në njësi sipas pattern-eve si:\n- 'FRAZA' / 'TERM' / 'Gabim'\n- 'Dialog'\n\n**TODO:** implementoni një segmentues të thjeshtë me regex.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import re\n\ndef segment_units(raw: str):\n    # Very simple heuristic splitter; improve it!\n    # Splits on headings like 'FRAZA', 'TERM', 'Dialog', 'Gabim'\n    pattern = r\"(?=\\b(FRAZA|TERM|Dialog|Gabim)\\b)\"\n    parts = re.split(pattern, raw)\n    # Re-stitch with label\n    units = []\n    i = 0\n    while i < len(parts)-1:\n        if parts[i] == \"\":\n            i += 1\n            continue\n        label = parts[i+1] if parts[i] in [\"FRAZA\",\"TERM\",\"Dialog\",\"Gabim\"] else None\n        if label:\n            content = parts[i+2].strip()\n            units.append(f\"{label} {content}\")\n            i += 3\n        else:\n            i += 1\n    return [u for u in units if len(u) > 50]\n\nunits = segment_units(text)\nprint(\"Units:\", len(units))\nfor u in units[:3]:\n    print(\"----\")\n    print(u[:400])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Embeddings me SentenceTransformers\nNdërtoni embeddings për 30 njësi dhe llogaritni similarity për disa pyetje.\n**TODO:** Shpjegoni pse unit-based embeddings janë më të mira se chunk-e arbitrare.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "!pip -q install sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nsample_units = units[:30] if len(units) >= 30 else units\nemb = model.encode(sample_units, normalize_embeddings=True)\nprint(\"Emb shape:\", emb.shape)\n\nquery = \"Çfarë do të thotë Roger në komunikim radio?\"\nq_emb = model.encode([query], normalize_embeddings=True)[0]\nscores = emb @ q_emb\ntop = np.argsort(-scores)[:5]\nfor idx in top:\n    print(scores[idx], sample_units[idx][:160])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Dorëzimi\n- Segmentuesi (regex + përmirësime)\n- Demonstrim similarity\n- Përgjigje: pse ky segmentim është i përshtatshëm për këtë projekt?\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "02_Tokenizimi_Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}