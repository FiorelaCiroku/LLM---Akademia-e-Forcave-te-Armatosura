{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Detyra 3 — Arkitektura Transformer\n\nQëllimi: të kuptoni tokenizimin, attention, dhe kufijtë e kontekstit në praktikë.\nKjo detyrë përdor një model të vogël (p.sh. `distilgpt2`) vetëm për demonstrim.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# === Setup ===\n!pip -q install transformers accelerate sentencepiece\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch, textwrap\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Tokenizimi i një dialogu\nZgjidhni një dialog nga dokumentet (tekst i ekstraktuar) ose shkruani një shembull.\nMatni numrin e tokenëve.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\nsample = \"Guard: Halt. Civilian: (stops) Guard: Identify yourself. Civilian: I am a local resident.\"\nids = tok(sample).input_ids\nprint(\"Tokens:\", len(ids))\nprint(ids[:20])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Si ndikon gjatësia e kontekstit\nShtoni tekst për ta bërë input-in më të gjatë dhe shihni truncation.\n**TODO:** shpjegoni pse chunking është i domosdoshëm.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "long_text = sample + \" \" + (\"Additional context. \" * 400)\nenc = tok(long_text, truncation=True, max_length=256, return_tensors=\"pt\")\nprint(\"Original chars:\", len(long_text))\nprint(\"Tokenized length:\", enc[\"input_ids\"].shape[1])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Demo: gjenerim i kontrolluar\nNuk kërkojmë cilësi të lartë: vetëm të shohim si modeli vazhdon tekstin.\n**TODO:** diskutoni pse na duhet RAG dhe guardrails.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\nprompt = \"Explain in Albanian what 'Roger' means in radio communication:\"\ninputs = tok(prompt, return_tensors=\"pt\")\nout = model.generate(**inputs, max_new_tokens=60, do_sample=True, top_p=0.9)\nprint(tok.decode(out[0], skip_special_tokens=True))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Dorëzimi\n- Screenshot ose output të token counts\n- Përgjigje me 5–8 fjali: si lidhen Transformers me RAG në këtë projekt.\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "03_Arkitektura_Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}